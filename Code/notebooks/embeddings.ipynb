{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Embeddings computation (node2vec)",
   "id": "39e98530ae09320a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T20:38:17.205069900Z",
     "start_time": "2026-01-09T20:38:15.371161200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "\n",
    "graph_path = \"../data/processed/amazon_graph.pickle\"\n",
    "with open(graph_path, \"rb\") as f:\n",
    "    G = pickle.load(f)"
   ],
   "id": "efd85b398d998301",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### CPU version",
   "id": "fb28aa8cbfb5a255"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import time\n",
    "import os\n",
    "from node2vec import Node2Vec"
   ],
   "id": "364bc9fabcdd1bc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_node2vec_emb_CPU(G, embedding_dim=128, walk_length=80, window=10,\n",
    "                         walks_per_node=10, p=1, q=2,):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes Node2Vec embeddings on the CPU using the standard `node2vec` library.\n",
    "\n",
    "    Args:\n",
    "        G (nx.Graph): Input NetworkX graph.\n",
    "        embedding_dim (int): Dimension of the output embedding vectors.\n",
    "        walk_length (int): Length of each random walk.\n",
    "        window (int): Window size for the skip-gram model.\n",
    "        walks_per_node (int): Number of random walks generated per node.\n",
    "        p (float): Return parameter (likelihood of returning to the immediate source).\n",
    "        q (float): In-out parameter (likelihood of moving away from the source).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing 'ASIN' (original ID) and embedding columns.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Starting embeddings computation on CPU using node2vec library\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Model configuration\n",
    "    node2vec_model = Node2Vec(\n",
    "        G,\n",
    "        dimensions=embedding_dim,\n",
    "        walk_length=walk_length,\n",
    "        num_walks=walks_per_node,\n",
    "        workers=os.cpu_count(),\n",
    "        p=p,\n",
    "        q=q,\n",
    "        quiet=False\n",
    "    )\n",
    "\n",
    "    # Model training\n",
    "    # window: max nodes distance at which the algorith will try to predict relations\n",
    "    # min_count: will consider also nodes that appear only 1 time\n",
    "    model = node2vec_model.fit(window = window, min_count = 1, batch_words = 4)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Tempo totale CPU: {end_time - start_time:.2f} secondi\")\n",
    "\n",
    "    # Output DataFrame building\n",
    "    df = pd.DataFrame(\n",
    "        index=model.wv.index_to_key,\n",
    "        data=model.wv.vectors\n",
    "    )\n",
    "\n",
    "    df.columns = [f\"emb_{i}\" for i in range(model.vector_size)]\n",
    "    df = df.reset_index() # push the index (ASIN) to be a standard column\n",
    "    df = df.rename(columns={'index': 'ASIN'}) # rename the \"index\" column\n",
    "\n",
    "    return df"
   ],
   "id": "dc0247b15864198c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embeddings = get_node2vec_emb_CPU(G)\n",
    "print(f\"DataFrame dimensions: {embeddings.shape}\")\n",
    "print(embeddings.head())\n",
    "\n",
    "embeddings.to_csv(\n",
    "    '../data/processed/embeddings_p1_q2.csv',\n",
    "    index=False,         # Row index is not needed\n",
    "    float_format='%.6f',\n",
    "    chunksize=10000\n",
    ")"
   ],
   "id": "ed276974d595b70d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "### GPU version"
   ],
   "id": "3fc30b0946817542"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-09T20:38:27.038104900Z",
     "start_time": "2026-01-09T20:38:20.849428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.nn import Node2Vec as torch_Node2Vec\n",
    "from torch_geometric.utils import from_networkx\n",
    "import pandas as pd"
   ],
   "id": "da34d3cb4225590",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-09T20:38:27.887024800Z",
     "start_time": "2026-01-09T20:38:27.855854900Z"
    }
   },
   "source": [
    "def get_node2vec_emb_GPU(G, embedding_dim=128,\n",
    "                         walk_length=80, context_size=10, walks_per_node=10,\n",
    "                         p=1, q=2, epochs=100, patience=3, batch_size=128):\n",
    "    \"\"\"\n",
    "    Computes Node2Vec embeddings using GPU acceleration via PyTorch Geometric.\n",
    "\n",
    "    This function preprocesses the input graph (attribute clearing and integer relabeling),\n",
    "    trains the model using an Early Stopping mechanism and re-maps the resulting vectors to\n",
    "    the original node IDs.\n",
    "\n",
    "    Args:\n",
    "        G (nx.Graph): Input NetworkX graph.\n",
    "        embedding_dim (int): Dimension of the output embedding vectors.\n",
    "        walk_length (int): Length of each random walk.\n",
    "        context_size (int): Window size for the skip-gram model.\n",
    "        walks_per_node (int): Number of random walks generated per node.\n",
    "        p (float): Return parameter (likelihood of returning to the immediate source).\n",
    "        q (float): In-out parameter (likelihood of moving away from the source).\n",
    "        epochs (int): Maximum number of training epochs.\n",
    "        patience (int): Epochs to wait for loss improvement before early stopping.\n",
    "        batch_size (int): Number of nodes per training batch.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the 'ASIN' (original node ID) and the\n",
    "                      corresponding embedding vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    if device == 'cuda':\n",
    "        print(\"CUDA available, using PyTorch on GPU.\")\n",
    "    if device == 'cpu':\n",
    "        print(\"CUDA not available, using PyTorch on CPU.\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Remove attributes from the graph's nodes\n",
    "    G_clean = G.copy()\n",
    "    for n in G_clean.nodes():       # for each node's ID n in G_clean...\n",
    "        G_clean.nodes[n].clear()    # access to the attributes dictionary\n",
    "\n",
    "    # Create a graph copy with integer node's labels\n",
    "    nodes_list = list(G_clean.nodes())\n",
    "    node_mapping = {node: i for i, node in enumerate(nodes_list)}\n",
    "    reverse_mapping = {i: node for i, node in enumerate(nodes_list)}\n",
    "    G_int = nx.relabel_nodes(G_clean, node_mapping)\n",
    "\n",
    "    # Convert the graph to PyTorch Geometric's Data object\n",
    "    data = from_networkx(G_int)\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Model configuration\n",
    "    model = torch_Node2Vec(\n",
    "        data.edge_index,\n",
    "        embedding_dim=embedding_dim,\n",
    "        walk_length=walk_length,\n",
    "        context_size=context_size,\n",
    "        walks_per_node=walks_per_node,\n",
    "        num_negative_samples=1,\n",
    "        p=p,\n",
    "        q=q,\n",
    "        sparse=True\n",
    "    ).to(device)\n",
    "\n",
    "    # Generates walks on-the-fly and set batches of 128 nodes\n",
    "    loader = model.loader(batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "    # set the appropriate optimizer since the matrix is sparse\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "\n",
    "    # Training\n",
    "    model.train()               # start training mode\n",
    "    best_loss = float('inf')    # + infinity\n",
    "    counter = 0\n",
    "\n",
    "    print(f\"Starting training\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        # for each nodes batch, read the pair of positive random walks and negative random walks\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch: {epoch+1:02d}, Loss: {total_loss / len(loader):.4f}\")\n",
    "\n",
    "        # Stopping logic\n",
    "        avg_loss = total_loss / len(loader)\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            counter = 0  # reset the counter since\n",
    "        else:\n",
    "            counter += 1\n",
    "            print(f\"   No improvement detected for {counter} epochs.\")\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Stopping model training since there has been no improvement for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Total GPU time: {end_time - start_time:.2f} s\")\n",
    "\n",
    "    # Embeddings extraction\n",
    "    model.eval()                    # stop training mode\n",
    "    with torch.no_grad():           # do not compute gradients\n",
    "        z = model().cpu().numpy()   # bring values to CPU\n",
    "\n",
    "    # Remap list index to the original node ID\n",
    "    emb_dict = {nodes_list[i]: z[i] for i in range(len(nodes_list))}\n",
    "    df = pd.DataFrame.from_dict(emb_dict, orient='index')\n",
    "\n",
    "    df.columns = [f\"emb_{i}\" for i in range(df.shape[1])]\n",
    "    df = df.reset_index() # push the index (ASIN) to be a standard column\n",
    "    df = df.rename(columns={'index': 'ASIN'}) # rename the \"index\" column\n",
    "\n",
    "    return df\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "embeddings = get_node2vec_emb_GPU(G)\n",
    "print(f\"DataFrame dimensions: {embeddings.shape}\")\n",
    "print(embeddings.head())\n",
    "\n",
    "embeddings.to_csv(\n",
    "    '../data/processed/embeddings_p1_q2.csv',\n",
    "    index=False,         # Row index is not needed\n",
    "    float_format='%.6f',\n",
    "    chunksize=10000\n",
    ")"
   ],
   "id": "4251a09b660aba41"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
