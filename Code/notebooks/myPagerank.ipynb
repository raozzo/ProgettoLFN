{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a35a3d0",
   "metadata": {},
   "source": [
    "# My Pagerank "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65ad11",
   "metadata": {},
   "source": [
    "## How?\n",
    "Pagerank implementation done from scratch using the *power iteration method*. In this method i tried to use specific-platform frameworks to speed up the computation\n",
    "## Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227a13b",
   "metadata": {},
   "source": [
    "# Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3353ae",
   "metadata": {},
   "source": [
    "## Platform identification\n",
    "This first section of code it's used to identify which platform we are using (generic Cpu, Apple Silicon or Nvidia GPU using cuda if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "193c9037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "def get_platform():\n",
    "    \"\"\"\n",
    "    Detects hardware in use and returns (platform_name, library_to_use ).\n",
    "    \"\"\"\n",
    "    if sys.platform == \"darwin\" and platform.processor() == \"arm\":\n",
    "        try:\n",
    "            import mlx.core as mx\n",
    "            return \"mlx\", mx\n",
    "        except ImportError:\n",
    "            pass\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            return \"cida\", torch\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return \"cpu\", np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71df341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlx\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(get_platform()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e3f3da",
   "metadata": {},
   "source": [
    "## PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8918851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prscores (graph, alpha = 0.85, tol= 1e-5, max_iter =1000):\n",
    "    '''\n",
    "    Compute PageRank Score \n",
    "    graph is the loaded pickle file \n",
    "    '''\n",
    "    \n",
    "    #preprocessing \n",
    "    if not sparse.isspmatrix_csc(graph):\n",
    "        #if not a compressed sparse column matrix we compress it\n",
    "        adj = graph.tocsc()\n",
    "    else: \n",
    "        #if already compressed we do nothing\n",
    "        adj = graph\n",
    "    \n",
    "    #number of nodes \n",
    "    n_nodes = adj.shape()\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        display(n_nodes)\n",
    "    \n",
    "    #we now can calcualate the out degree of each node (that given the adjaceny matrix \n",
    "    # is simply the sum along colums )\n",
    "    out_degrees = np.array(adj.sum(axis=0)).flatten()\n",
    "    is_sink = (out_degree == 0)                 #another check if a node is a sink\n",
    "    \n",
    "    # Normalize transition probabilities: P_ij = 1 / out_degree(j) (ignoring sinks)\n",
    "    norm_out_degrees = np.where(is_sink, 1.0, out_degrees)\n",
    "    adj.data = adj.data / norm_out_degrees[adj.indices]\n",
    "    \n",
    "    platform_name, engine = get_paltform()\n",
    "    \n",
    "    print(\"for now we always use cpu \")\n",
    "    \n",
    "    #we create the pagerank vector (initialized as every node as probability 1/n)\n",
    "    pr = np.full(n_nodes, 1.0/n_nodes)\n",
    "    teleport_const = (1.0 - alpha) / n_nodes\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        r_next = adj.dot(r)\n",
    "        sink_mass = np.sum(r[is_sink])\n",
    "        r_next = alpha*(r_next +sink_mass / n_nodes)+teleport_const\n",
    "        \n",
    "        # check if maximum difference is lower than tol, if yes not much imprvement so we break\n",
    "        if np.linalg.norm(r_next - r,1)<tol:\n",
    "            break\n",
    "        #else we update the pagerank vector \n",
    "        r = r_next\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e092af2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5efbeb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LFN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
