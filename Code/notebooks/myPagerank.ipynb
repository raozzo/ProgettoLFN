{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a35a3d0",
   "metadata": {},
   "source": [
    "# My Pagerank "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65ad11",
   "metadata": {},
   "source": [
    "## How?\n",
    "Pagerank implementation done from scratch using the *power iteration method*. In this method i tried to use specific-platform frameworks to speed up the computation\n",
    "\n",
    "- *generic Cpu* \n",
    "- *apple silicon*\n",
    "- *GPU*: for this we used torch implementation die to the faster multiplication on sparse matrix  \n",
    "## Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4227a13b",
   "metadata": {},
   "source": [
    "# Code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3353ae",
   "metadata": {},
   "source": [
    "## Platform identification\n",
    "This first section of code it's used to identify which platform we are using (generic Cpu, Apple Silicon or Nvidia GPU using cuda if present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "193c9037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import sys\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def get_platform():\n",
    "    \"\"\"\n",
    "    Detects hardware in use and returns (platform_name, library_to_use ).\n",
    "    \"\"\"\n",
    "    if sys.platform == \"darwin\" and platform.processor() == \"arm\":\n",
    "        try:\n",
    "            import mlx.core as mx\n",
    "            return \"mlx\", mx\n",
    "        except ImportError:\n",
    "            pass\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            return \"gpu\", torch\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return \"cpu\", np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71df341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlx\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(get_platform()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e3f3da",
   "metadata": {},
   "source": [
    "## PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8918851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prscores (graph_obj, alpha = 0.85, tol= 1e-5, max_iter =1000):\n",
    "    '''\n",
    "    Compute PageRank Score \n",
    "    graph is the loaded pickle file \n",
    "    '''\n",
    "    \n",
    "    # Extract Adjacency Matrix\n",
    "    if hasattr(graph_obj, 'adjacency'):\n",
    "        node_labels = list(graph_obj.nodes())\n",
    "        # Convert to Scipy CSC matrix \n",
    "        adj = nx.to_scipy_sparse_array(graph_obj, format='csc', dtype=np.float32)\n",
    "    else:\n",
    "        adj = graph_obj.tocsc()\n",
    "        node_labels = list(range(adj.shape))\n",
    "    \n",
    "    #number of nodes \n",
    "    n_nodes = adj.shape[0]\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        display(n_nodes)\n",
    "    \n",
    "    #we now can calcualate the out degree of each node (that given the adjaceny matrix \n",
    "    # is simply the sum along colums )\n",
    "    out_degrees = np.array(adj.sum(axis=1)).flatten()\n",
    "    is_sink = (out_degrees == 0)                 #another check if a node is a sink\n",
    "    \n",
    "    # Normalize transition probabilities: P_ij = 1 / out_degree(j) (ignoring sinks)\n",
    "    norm_out_degrees = np.where(is_sink, 1.0, out_degrees)\n",
    "    adj.data = adj.data / norm_out_degrees[np.repeat(np.arange(n_nodes), np.diff(adj.indptr))]\n",
    "    \n",
    "    platform_name, engine = get_platform()\n",
    "    \n",
    "    # PageRank uses the transpose for 'Pull' aggregation: r_next = alpha * (adj.T @ r)\n",
    "    P_matrix = adj.T.tocsc()\n",
    "    \n",
    "    #NOTE change it to mlx \n",
    "    if platform_name == \"mlx\":\n",
    "        import mlx.core as mx\n",
    "        # Pre-calculate target indices using NumPy (MLX repeat doesn't support arrays yet)\n",
    "        counts = np.diff(P_matrix.indptr)\n",
    "        targets_np = np.repeat(np.arange(n_nodes), counts)\n",
    "        \n",
    "        # Move arrays to Unified Memory\n",
    "        indices = mx.array(P_matrix.indices)\n",
    "        data = mx.array(P_matrix.data)\n",
    "        targets = mx.array(targets_np)\n",
    "        sink_mask = mx.array(is_sink.astype(np.float32))\n",
    "        \n",
    "        r = mx.full((n_nodes,), 1.0 / n_nodes)\n",
    "        teleport_v = (1.0 - alpha) / n_nodes\n",
    "\n",
    "        @mx.compile\n",
    "        def update_step(r_prev):\n",
    "            # Weighted values to sum\n",
    "            weighted = data * r_prev[indices]\n",
    "            \n",
    "            # CORRECT MLX SYNTAX: Use.at.add() for parallel scatter-add\n",
    "            res = mx.zeros((n_nodes,))\n",
    "            res = res.at[targets].add(weighted)\n",
    "            \n",
    "            # Sink correction\n",
    "            sink_mass = mx.sum(r_prev * sink_mask)\n",
    "            return (alpha * (res + sink_mass / n_nodes)) + teleport_v\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            r_next = update_step(r)\n",
    "            mx.eval(r_next) # Materialize the lazy graph\n",
    "            \n",
    "            if mx.sum(mx.abs(r_next - r)) < tol:\n",
    "                print(f\"Converged at iteration {i}\")\n",
    "                break\n",
    "            r = r_next\n",
    "        final_ranks = np.array(r)\n",
    "    \n",
    "    \n",
    "    elif platform_name == \"gpu \":\n",
    "        import torch\n",
    "        device = torch.device(\"cuda\")\n",
    "        P_torch = torch.sparse_csc_tensor(\n",
    "            torch.from_numpy(P_matrix.indptr).to(torch.int64),\n",
    "            torch.from_numpy(P_matrix.indices).to(torch.int64),\n",
    "            torch.from_numpy(P_matrix.data).to(torch.float32),\n",
    "            size=(n_nodes, n_nodes)\n",
    "        ).to(device)\n",
    "        \n",
    "        sinks = torch.from_numpy(is_sink).to(device)\n",
    "        pr = torch.full((n_nodes, 1), 1.0 / n_nodes, device=device)\n",
    "        teleport_v = (1.0 - alpha) / n_nodes\n",
    "\n",
    "        for i in range(max_iter):\n",
    "            #Sparse MAtrix_vect multiplication (more efficient)\n",
    "            pr_next = torch.sparse.mm(P_torch, r)\n",
    "            sink_mass = torch.sum(r[sinks])\n",
    "            pr_next = alpha * (pr_next + sink_mass / n_nodes) + teleport_v\n",
    "            \n",
    "            if torch.norm(pr_next - pr, p=1) < tol:\n",
    "                print(f\"Converged at iteration {i}\")\n",
    "                break\n",
    "            pr = pr_next\n",
    "        #NOTE cpu() transfer from VRAM ----> RAM     \n",
    "        final_ranks = pr.cpu().numpy().flatten()\n",
    "    \n",
    "    else:\n",
    "        #we create the pagerank vector (initialized as every node as probability 1/n)\n",
    "        pr = np.full(n_nodes, 1.0/n_nodes)\n",
    "        teleport_const = (1.0 - alpha) / n_nodes\n",
    "        for i in range(max_iter):\n",
    "            \n",
    "            pr_next = P_matrix.dot(pr)\n",
    "            sink_mass = np.sum(pr[is_sink])\n",
    "            pr_next = alpha*(pr_next +sink_mass / n_nodes)+teleport_const\n",
    "            \n",
    "            # check if maximum difference is lower than tol, if yes not much imprvement so we break\n",
    "            if np.linalg.norm(pr_next - pr,1)<tol:\n",
    "                break\n",
    "            #else we update the pagerank vector \n",
    "            pr = pr_next\n",
    "            \n",
    "        final_ranks = pr\n",
    "        \n",
    "        \n",
    "    return dict(zip(node_labels, final_ranks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a07b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f60fb47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LFN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
