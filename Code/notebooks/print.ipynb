{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e067ee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import networkx as nx\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02413ecf",
   "metadata": {},
   "source": [
    "# Structural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "086bd51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src.features import get_pagerank, get_approx_betweenness, get_clustering_coefficient, get_harmonic_centrality\n",
    "\n",
    "from src import load_or_compute\n",
    "\n",
    "# 1. Load the graph from the pickle file\n",
    "# Make sure the path is correct relative to where you run this script\n",
    "with open(\"../data/processed/amazon_graph.pickle\", \"rb\") as f:\n",
    "    G_loaded = pickle.load(f)\n",
    "\n",
    "recompute = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e5702b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing\n",
      "cpu\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'operation_type' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_handpagerank \u001b[38;5;241m=\u001b[39m \u001b[43mload_or_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/processed/pagerank_scores.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_pagerank\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mG_loaded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_cpu\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Now all functions (build_index_map, calculate_pagerank, etc.) are available in memory\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#prscores = get_prscores(G_loaded)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m display(\u001b[38;5;28msum\u001b[39m(df_handpagerank[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpagerank\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues))\n",
      "File \u001b[0;32m~/ProgettoLFN/Code/src/utils.py:80\u001b[0m, in \u001b[0;36mload_or_compute\u001b[0;34m(file_path, compute_func, force_recompute, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m#NOTE: for now on pagerak there is no method to save platform\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m \u001b[43mlog_times\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Save the result\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving to \u001b[39m\u001b[38;5;132;01m{file_path}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ProgettoLFN/Code/src/utils.py:24\u001b[0m, in \u001b[0;36mlog_times\u001b[0;34m(func_name, duration_sec, params)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlog_times\u001b[39m(func_name, duration_sec, params):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    Append to CSV (or create it if not existant) where the entries are the timestamp, function name, function and function args\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    The timestamp is probably not necessary but it's for now a way to not overlap logs made in different computer\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     entry \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m'\u001b[39m: func_name,\n\u001b[0;32m---> 24\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moperation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43moperation_type\u001b[49m, \u001b[38;5;66;03m# 'Computed' or 'Loaded'\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration_sec\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mround\u001b[39m(duration_sec, \u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(params) \u001b[38;5;66;03m# Save kwargs as string\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     }\n\u001b[1;32m     29\u001b[0m     df_entry\u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([entry])\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(TIME_FILE):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'operation_type' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "df_handpagerank = load_or_compute(\"../data/processed/pagerank_scores.csv\", get_pagerank , True , G = G_loaded, force_cpu = False)\n",
    "# Now all functions (build_index_map, calculate_pagerank, etc.) are available in memory\n",
    "#prscores = get_prscores(G_loaded)\n",
    "display(sum(df_handpagerank[\"pagerank\"].values))\n",
    "#df_handpagerank = pd.DataFrame(list(prscores.items()), columns=['ASIN', 'MyHandPageRank'])\n",
    "display(df_handpagerank.head(20))\n",
    "# Save to CSV\n",
    "#df_handpagerank.to_csv(\"../data/processed/handpagerank_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9b33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_bet = load_or_compute(\"../data/processed/bet_scores.csv\",get_approx_betweenness,recompute,G=G_loaded,k=1000) \n",
    "display(df_bet.head(5))\n",
    "# Save to CSV\n",
    "\n",
    "\n",
    "\n",
    "# Now you can use the scores\n",
    "print(f\"Computed bet cent for {len(df_bet)} nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3feb789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_clus = load_or_compute(\"../data/processed/clus_scores.csv\",get_clustering_coefficient,recompute,G = G_loaded)\n",
    "display(df_clus.head(5))\n",
    "\n",
    "# Now you can use the scores\n",
    "print(f\"Computed cc for {len(df_bet)} nodes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b186f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Harmonic scores and save them to a csv file\n",
    "df_harmonic_scores = load_or_compute(\"../data/processed/harm_scores.csv\",get_harmonic_centrality,recompute,G= G_loaded, version=\"GPU\")\n",
    "#df_harmonic_scores.to_csv(\"../data/processed/harm_scores.csv\", index=False)\n",
    "\n",
    "display(df_harmonic_scores.head(5))\n",
    "print(f\"Computed hc for {len(df_harmonic_scores)} nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf8b54",
   "metadata": {},
   "source": [
    "### Now we work on df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c7f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_pagerank = pd.DataFrame.from_dict(pagerank_scores, orient='index', columns=['PageRank'])\n",
    "#df_clus = pd.DataFrame.from_dict(clus_scores, orient='index', columns=['ClusteringCoeff'])\n",
    "#df_bet = pd.DataFrame.from_dict(bet_scores, orient='index', columns=['Betweenness'])\n",
    "\n",
    "\n",
    "data_frames = [df_handpagerank,df_clus,df_bet,df_harmonic_scores]\n",
    "\n",
    "df_final = pd.concat(data_frames,axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fee0295",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "salesrank_dict = nx.get_node_attributes(G_loaded, 'salesrank')\n",
    "\n",
    "# 2. Convert to DataFrame\n",
    "df_salesrank = pd.DataFrame.from_dict(salesrank_dict, orient='index', columns=['salesrank'])\n",
    "df_salesrank.index.name = 'ASIN'\n",
    "\n",
    "# 3. Join to df_final\n",
    "# This performs a left join on the index (ASIN)\n",
    "df_final = df_final.join(df_salesrank)\n",
    "\n",
    "# Check the result\n",
    "display(df_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cddc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"../data/processed/structural_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bffcc38",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "features = ['pagerank','Betweenness','ClusteringCoefficient','HarmonicCentrality','salesrank']\n",
    "X=df_final[features]\n",
    "\n",
    "#non ci dovrebbero essere NaN ma per sicurezza\n",
    "X=X.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7fa28",
   "metadata": {},
   "source": [
    "Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec58ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df_final['cluster_kmeans']= clusters\n",
    "\n",
    "#questo stampa quanti nodi per ogni gruppo\n",
    "print(df_final['cluster_kmeans'].value_counts())\n",
    "\n",
    "#maybe we can save it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f030775",
   "metadata": {},
   "source": [
    "confronto con gruppi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ee915",
   "metadata": {},
   "outputs": [],
   "source": [
    "#aggiungo il gruppo \n",
    "groups_dict = nx.get_node_attributes(G_loaded, 'group')\n",
    "\n",
    "# 2. Converti in un DataFrame (assicurandoti che l'indice sia l'ASIN)\n",
    "df_groups = pd.DataFrame.from_dict(groups_dict, orient='index', columns=['group'])\n",
    "df_groups.index.name = 'ASIN'\n",
    "\n",
    "# 3. Unisci al tuo df_final\n",
    "# join Ã¨ intelligente: allinea automaticamente gli indici (ASIN)\n",
    "df_final = df_final.join(df_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d151cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "crosstab = pd.crosstab(df_final['cluster_kmeans'], df_final['group'])\n",
    "\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe541f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(crosstab, annot=True, fmt='d',cmap='YlGnBu')\n",
    "plt.xlabel('Categoria Originale')\n",
    "plt.ylabel('Cluster Assegnato')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3c0de",
   "metadata": {},
   "source": [
    "# Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e420ad",
   "metadata": {},
   "source": [
    "TODO: call load or compute or a similar function\n",
    "load the embeddings in a df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635547bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file_path = \"../data/processed/embeddings.csv\" \n",
    "df_embeddings = pd.read_csv(embedding_file_path, index_col=0)\n",
    "\n",
    "print(\"embedding dimension:\", df_embeddings.shape)\n",
    "display(df_embeddings.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665cf8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, normalize\n",
    "\n",
    "X = df_embeddings.values\n",
    "\n",
    "X_norm = normalize(X) \n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(X_norm)\n",
    "\n",
    "df_embeddings['cluster_kmeans'] = clusters\n",
    "\n",
    "print(\"\\nConteggio Cluster:\")\n",
    "print(df_embeddings['cluster_kmeans'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcde3a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.join(df_embeddings['cluster_kmeans'], rsuffix='_emb')\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f2b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab = pd.crosstab(df_final['cluster_kmeans_emb'], df_final['group'])\n",
    "\n",
    "print(crosstab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9869a90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(crosstab, annot=True, fmt='d',cmap='YlGnBu')\n",
    "plt.xlabel('Categoria Originale')\n",
    "plt.ylabel('Cluster Assegnato')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9accf9e8",
   "metadata": {},
   "source": [
    "## Hybrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e471dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frames = [df_handpagerank,df_clus,df_bet,df_harmonic_scores,df_salesrank]\n",
    "\n",
    "df_struct = pd.concat(data_frames,axis=1)\n",
    "df_struct.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3852de",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_struct = scaler.fit_transform(df_final[['pagerank', 'Betweenness', 'ClusteringCoefficient', 'HarmonicCentrality', 'salesrank']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ho ricaricaricato gli embeddings ma si puo usare il df di prima \n",
    "df_emb = pd.read_csv(\"../data/processed/embeddings.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41d9b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hybrid_raw = df_struct.join(df_emb, how='inner', lsuffix='_struct', rsuffix='_emb')\n",
    "\n",
    "# Separiamo le colonne per poter applicare normalizzazioni diverse\n",
    "cols_struct = df_struct.columns\n",
    "cols_emb = df_emb.columns\n",
    "\n",
    "# Se hai colonne 'non-feature' (come 'group' o 'title'), rimuovile dalle liste\n",
    "cols_struct = [c for c in cols_struct if c not in ['group', 'title', 'ASIN']]\n",
    "# cols_emb dovrebbe contenere solo numeri (0, 1, ... 127)\n",
    "\n",
    "print(f\"Feature Strutturali: {len(cols_struct)}\")\n",
    "print(f\"Dimensioni Embedding: {len(cols_emb)}\")\n",
    "print(f\"Totale Nodi Allineati: {len(df_hybrid_raw)}\")\n",
    "\n",
    "\n",
    "\n",
    "# Opzione A: MinMax Scaling (Tutto tra 0 e 1) - Spesso preferito per ibridi\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "# Opzione B: StandardScaler (Media 0, Var 1)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Applichiamo lo scaler all'intero dataset concatenato\n",
    "# (Oppure puoi scalare le due parti separatamente se vuoi pesi diversi)\n",
    "X_hybrid = scaler.fit_transform(df_hybrid_raw[cols_struct + list(cols_emb)].fillna(0))\n",
    "\n",
    "\n",
    "df_hybrid_features = pd.DataFrame(\n",
    "    X_hybrid, \n",
    "    index=df_hybrid_raw.index, \n",
    "    columns=list(cols_struct) + list(cols_emb)\n",
    ")\n",
    "\n",
    "display(df_hybrid_features.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4abe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(df_hybrid_features)\n",
    "\n",
    "# Aggiungiamo i cluster al df originale per analisi\n",
    "#df_results = df_hybrid_raw.copy()\n",
    "df_final['cluster_hybrid'] = clusters\n",
    "\n",
    "# Se hai la colonna 'group' (ground truth), analizza i risultati\n",
    "if 'group' in df_final.columns:\n",
    "    print(\"\\nConfronto con Ground Truth:\")\n",
    "    print(pd.crosstab(df_final['cluster_hybrid'], df_final['group']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33084e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa6a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(crosstab, annot=True, fmt='d',cmap='YlGnBu')\n",
    "plt.xlabel('Categoria Originale')\n",
    "plt.ylabel('Cluster Assegnato')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49727af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LFN_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
